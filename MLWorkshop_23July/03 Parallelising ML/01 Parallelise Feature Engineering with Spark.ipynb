{"cells":[{"cell_type":"markdown","source":["#Understanding Parallelization of Machine Learning Algorithms in Apache Spark™\n\n##Data Preparation"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6efb448f-8f12-46d8-8e6e-86776ea27999"}}},{"cell_type":"markdown","source":["The dataset used for this example is Bank marketing. Given a set of features about a customer can we predict whether the person will open a term deposit account.\n\nOriginal Source: [UCI Machine Learning Repository \nBank Marketing Data Set](https://archive.ics.uci.edu/ml/datasets/bank+marketing)\n[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2dde37dd-b727-40f2-a6d6-419200fa9284"}}},{"cell_type":"markdown","source":["####Attribute Information:\n\n####Input variables:\nbank client data:\n```\n1 - age (numeric)\n2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n5 - default: has credit in default? (categorical: 'no','yes','unknown')\n6 - housing: has housing loan? (categorical: 'no','yes','unknown')\n7 - loan: has personal loan? (categorical: 'no','yes','unknown')\nrelated with the last contact of the current campaign:\n8 - contact: contact communication type (categorical: 'cellular','telephone') \n9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\nother attributes:\n12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n14 - previous: number of contacts performed before this campaign and for this client (numeric)\n15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\nsocial and economic context attributes\n16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)\n17 - cons.price.idx: consumer price index - monthly indicator (numeric) \n18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric) \n19 - euribor3m: euribor 3 month rate - daily indicator (numeric)\n20 - nr.employed: number of employees - quarterly indicator (numeric)\n\nOutput variable (desired target):\n21 - y - has the client subscribed a term deposit? (binary: 'yes','no')\n```"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbe7e0c2-d0ff-46b0-89e8-d5e4a5fea089"}}},{"cell_type":"code","source":["#Run this cell to download the dataset\ndbutils.notebook.run(\"../Utils/01 Get Bank Marketing Dataset\",0)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1437bae1-625b-4854-aa2e-d4289b537417"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(table(\"bank_marketing\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ae0f1a39-f446-4fc3-bd89-6ba8dd5f587d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Feature Engineering in Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d0536956-0414-43ae-b6bc-d7da8b1348f2"}}},{"cell_type":"code","source":["input_data = table(\"bank_marketing\")\n\ncols = input_data.columns"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d45a5a23-2ede-4e70-907d-8b1765c6028a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, StringIndexerModel, VectorAssembler\n\ncategoricalColumns = [\"job\", \"marital\", \"education\", \"default\", \"housing\", \"loan\", \"contact\", \"month\", \"poutcome\"]\nstages = [] # stages in our Pipeline\nfor categoricalCol in categoricalColumns:\n  # Category Indexing with StringIndexer\n  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n  # Use OneHotEncoder to convert categorical variables into binary SparseVectors\n  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n  # Add stages.  These are not run here, but will be run all at once later on.\n  stages += [stringIndexer, encoder]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd991f00-d2be-43a5-8f09-9d0bbfb65e95"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#numericCols = [\"age\", \"balance\", \"duration\", \"campaign\", \"previous\", \"day\"]\nnumericCols = [\"age\", \"balance\", \"campaign\", \"previous\", \"day\"]\n\nassemblerInputs = numericCols + [c + \"classVec\" for c in categoricalColumns]\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d88b527d-952e-4d16-97cc-98ce03ceeec0"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["labelIndexer = StringIndexer(inputCol=\"y\", outputCol=\"label\")\n\nstages += [labelIndexer]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"153eca93-1886-406f-a166-12267551ef22"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["pipeline = Pipeline(stages=stages)\n# Run the feature transformations.\n#  - fit() computes feature statistics as needed.\n#  - transform() actually transforms the features.\npipelineModel = pipeline.fit(input_data)\ndataset = pipelineModel.transform(input_data)\n\n# Keep relevant columns\nselectedcols = [\"label\", \"features\"] + cols\ndataset = dataset.select(selectedcols)\ndisplay(dataset)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4d95986c-f6a2-4481-b6d4-3b958fcc1dca"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Write out the Training and Testing data for future reference"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9c6e1972-6bdf-4b68-b548-79b1651e4b10"}}},{"cell_type":"code","source":["(trainingData, testData) = dataset.randomSplit([0.8, 0.2], seed = 12)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bc9246ad-31b8-4fbf-9040-ada9aa41b602"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\n// Set the username\nval tags = com.databricks.logging.AttributionContext.current.tags\nval name = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_USER, java.util.UUID.randomUUID.toString.replace(\"-\", \"\"))\nval username = if (name != \"unknown\") name else dbutils.widgets.get(\"databricksUsername\")\nspark.conf.set(\"my.username\", username)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ba267ad5-5952-4f24-b7fe-3dd1eb2840e1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["username = spark.conf.get(\"my.username\")\ntrainingData.write.mode(\"overwrite\").format(\"delta\").save(\"dbfs:/ml-workshop-datasets/{}/employee/delta/singleNode/trainingData\".format(username))\ntestData.write.mode(\"overwrite\").format(\"delta\").save(\"dbfs:/ml-workshop-datasets/{}/employee/delta/singleNode/testData\".format(username))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e316c0d4-abe9-4df5-9828-2199365a35b2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Read data in and convert to Pandas dataframe"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"35078320-ae9e-4ded-8290-aabdd95bd8c1"}}},{"cell_type":"code","source":["pdf_train = (spark.read\n             .format(\"delta\")\n             .load(\"dbfs:/ml-workshop-datasets/{}/employee/delta/singleNode/trainingData\".format(username))\n             .toPandas())\n\npdf_test = (spark.read\n            .format(\"delta\")\n            .load(\"dbfs:/ml-workshop-datasets/{}/employee/delta/singleNode/testData\".format(username))\n            .toPandas())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"812fc57a-ccaa-41c7-b0ad-1c183fec5984"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["pdf_train"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bbc27d64-c591-4a75-ad8a-ba8f45b910b1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Ready to train your model!"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96423b6b-85bd-4adb-8dfa-13101efc2819"}}},{"cell_type":"code","source":["from sklearn.ensemble import RandomForestClassifier\n\nrf = RandomForestClassifier()\nrf.fit(pdf_train.iloc[:, 1:], pdf_train[\"label\"])\npredictions = rf.predict(pdf_test.iloc[:, 1:])\n\npredictions"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"505cb72d-2328-41be-aabe-7b9e76c4df05"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"01 Parallelise Feature Engineering with Spark","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3256439173735910}},"nbformat":4,"nbformat_minor":0}
