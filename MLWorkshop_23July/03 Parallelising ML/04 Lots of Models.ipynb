{"cells":[{"cell_type":"markdown","source":["#Understanding Parallelization of Machine Learning Algorithms in Apache Spark™\n\n##Pandas_UDFs for training lots of models in Parallel"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f124be3f-5a65-4002-b2e3-0e481e85e107"}}},{"cell_type":"markdown","source":["The dataset used for this example is Bank marketing. Given a set of features about a customer can we predict whether the person will open a term deposit account.\n\nOriginal Source: [UCI Machine Learning Repository \nBank Marketing Data Set](https://archive.ics.uci.edu/ml/datasets/bank+marketing)\n[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe9fc022-7b63-414e-929e-345b4d11cf54"}}},{"cell_type":"code","source":["train_data_path_2 = \"dbfs:/ml-workshop-datasets/employee/delta/trainingData\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48e70951-14e3-408e-9eb9-86ad635f6a7a"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%scala\nval tags = com.databricks.logging.AttributionContext.current.tags\nval name = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_USER, java.util.UUID.randomUUID.toString.replace(\"-\", \"\"))\nvar username = if (name != \"unknown\") name else dbutils.widgets.get(\"databricksUsername\")\nspark.conf.set(\"my.fq_username\", username)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d1fbeea-bacf-4fe4-add4-f0433c62ef05"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["username = spark.conf.get(\"my.fq_username\")\nuser_name = username.replace(\"@databricks.com\", \"\").replace(\".\",\"_\")\nprint(\"User Name:\", user_name)\n\nmlflow_exp = \"/Users/\"+username+\"/example_experiment\"\nprint(\"MLFlow Experiment:\", mlflow_exp)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bd02a58f-7272-4ebc-ab62-c8314711f03d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 1. Read in the training data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ffdc3de-4de0-4f87-b9d8-8b63fd1db070"}}},{"cell_type":"code","source":["trainingData = spark.read.format(\"delta\").load(train_data_path_2)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f3d2d136-b783-476d-9097-7b50e6b0605c"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(trainingData)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7b6dc134-5fc4-44b1-a429-53e88822305c"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. ML Flow Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2d012976-42dc-45de-b590-83341040a5f6"}}},{"cell_type":"code","source":["# ML Flow in pandas udf\nimport mlflow\ncntx = dbutils.entry_point.getDbutils().notebook().getContext()\napi_token = cntx.apiToken().get()\napi_url = cntx.apiUrl().get()\nmlflow.set_tracking_uri(\"databricks\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"19f7698f-1448-41ae-9f78-5eac06a59139"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Define the function to run on each campaign group"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"18d594cb-1bcb-49fd-ab53-e31a9994aec9"}}},{"cell_type":"code","source":["# We need to define the return schema\nfrom pyspark.sql.functions import pandas_udf, PandasUDFType\nfrom pyspark.sql.types import DoubleType, StructType, StructField\nschema = StructType([StructField('Predictions', DoubleType(), True), \n                     StructField('campaign', DoubleType(), True)\n                    ])\n\n@pandas_udf(schema, PandasUDFType.GROUPED_MAP)\ndef train_rf_groups(data):\n  # Library imports\n  import pandas as pd\n  import mlflow\n  import mlflow.sklearn\n  import os\n  import sklearn\n  from sklearn.model_selection import train_test_split\n  from sklearn.ensemble import RandomForestClassifier\n  from sklearn.metrics import roc_auc_score\n  \n  \n  # ML Flow setup\n  os.environ[\"DATABRICKS_TOKEN\"] = api_token\n  os.environ[\"DATABRICKS_HOST\"] = api_url\n  mlflow.set_experiment(mlflow_exp)\n  \n  # get relevant info from the input dataframe\n  campaign = data['campaign'].iloc[0]\n  \n  # Train, test split\n  train, test = train_test_split(data)\n\n  # The predicted column is \"label\" \n  train_x = train[[\"age\",\"balance\",\"previous\",\"day\",\"duration\", \"pdays\"]]\n  test_x = test[[\"age\",\"balance\",\"previous\",\"day\",\"duration\", \"pdays\"]]\n  train_y = train[[\"label\"]]\n  test_y = test[[\"label\"]]\n  \n  # log the run into mlflow\n  with mlflow.start_run() as run:\n    # could also apply feature engineering steps here as well\n    \n    rf = RandomForestClassifier(n_estimators=100, max_depth=7,random_state=0)\n    rf.fit(train_x, train_y)\n\n    predicted_qualities = pd.DataFrame(rf.predict(test_x), columns=[\"Predictions\"])\n    auc = roc_auc_score(test_y, predicted_qualities)\n    \n    # Log mlflow attributes for mlflow UI\n    mlflow.log_param(\"max_depth\", 7)\n    mlflow.log_param(\"n_estimators\", 100)\n    mlflow.log_metric(\"ROC AUC\", auc)\n    mlflow.sklearn.log_model(rf, \"model\")\n    \n  predicted_qualities['campaign'] = data['campaign'][0]\n    \n  return predicted_qualities\n  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1805328f-44bc-4f35-86e3-9fd65f3566d1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Perform some cleaning steps on the data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28ec03ed-3b48-4d0f-b508-7791cee76509"}}},{"cell_type":"code","source":["# First, limit to those campaigns which have sufficient data for a model\n# find the relevant campaign ids\nfrom pyspark.sql.functions import count, col\ncampaigns = trainingData.groupby(\"campaign\").agg(count(\"campaign\")).where(col(\"count(campaign)\")>300).select(\"campaign\")\n\n# now limit the training data just to those campaigns\ntrainingData_campaigns = trainingData.join(campaigns, \"campaign\")\ndisplay(trainingData_campaigns)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f39e0988-8a8a-41dc-aa18-3548f9bfecfc"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Can't convert a Spark feature vector to pandas here, everything needs to be done in the pandas_udf\ntrainingData_campaigns = trainingData_campaigns.drop(\"features\")\ndisplay(trainingData_campaigns)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dff5165a-0dda-44d6-ad92-133be15732a3"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(trainingData_campaigns.groupBy(\"campaign\").count())"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0c7a8729-ea2b-4950-b5c0-e2579057515d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 4. Apply the function to the different campaign groups"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7eb233e1-501e-47a2-8216-f7be9a9a5117"}}},{"cell_type":"code","source":["output = trainingData_campaigns.groupby(\"campaign\").apply(train_rf_groups)\ndisplay(output)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8eb7e813-510f-4f81-b306-14b4c0bf0b74"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import time\nmodel_name = \"<FILL IN>\"\nclient = mlflow.tracking.MlflowClient()\nregistered_model = client.get_registered_model(model_name)\nexp_id = client.get_experiment_by_name(mlflow_exp).experiment_id\nruns = mlflow.search_runs(exp_id)\nartifact_uri=runs[\"artifact_uri\"][0]\nmodel_version = client.create_model_version(model_name, f\"{artifact_uri}/model\", runs[\"run_id\"][0])\ntime.sleep(5)\nclient.update_model_version(model_name, model_version.version, stage=\"Production\", description=\"My next prod version\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"Finally, register one of these models to our demo...","showTitle":true,"inputWidgets":{},"nuid":"9d1abb60-1985-40f5-917e-642b9b67e487"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"4516a9c0-c480-49e6-adbe-44672629247e"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"04 Lots of Models","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3256439173735889}},"nbformat":4,"nbformat_minor":0}
