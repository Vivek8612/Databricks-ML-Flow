{"cells":[{"cell_type":"markdown","source":["##<img src=\"https://databricks.com/wp-content/themes/databricks/assets/images/header_logo_2x.png\" alt=\"logo\" width=\"150\"/> \n### Feature Engineering for Distributed HyperOpt\n\n##### Pre-requisites:\n* Data\n  * dbfs:/ml-workshop-datasets/employee/delta/trainingData\n  * dbfs:/ml-workshop-datasets/employee/delta/testData\n  \n#### Notebook overview\n\nThe structure of the notebook is as follows:\n- Settings\n- Data preparation logic\n  - Data description recap\n  - Read data with Spark\n  - Handle missing values, and generate features using Spark pipelines\n  - Convert data to Pandas\n  - Write data to DBFS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fd524b51-7f4f-45ce-8b4a-73667d6e80f4"}}},{"cell_type":"markdown","source":["The dataset used for this example is Bank marketing. Given a set of features about a customer can we predict whether the person will open a term deposit account.\n\nOriginal Source: [UCI Machine Learning Repository \nBank Marketing Data Set](https://archive.ics.uci.edu/ml/datasets/bank+marketing)\n[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"02eca425-b26c-479d-801a-986f600a875b"}}},{"cell_type":"code","source":["train_data_path = \"/ml-workshop-datasets/employee/delta/trainingData\"\ntest_data_path = \"/ml-workshop-datasets/employee/delta/testData\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"349995de-4139-4c70-b6fa-0da8dd3d2c4e"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## Settings"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6f27aa74-fcc1-44a2-be74-1ac185f4d755"}}},{"cell_type":"code","source":["%scala\n// Set the username\nval tags = com.databricks.logging.AttributionContext.current.tags\nval name = tags.getOrElse(com.databricks.logging.BaseTagDefinitions.TAG_USER, java.util.UUID.randomUUID.toString.replace(\"-\", \"\"))\nval username = if (name != \"unknown\") name else dbutils.widgets.get(\"databricksUsername\")\nspark.conf.set(\"my.username\", username)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e917d7ce-cc50-42eb-8d37-e157b04e7637"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import os\n\nusername = spark.conf.get(\"my.username\")\nname = username.replace(\"@databricks.com\", \"\").replace(\".\",\"_\")\n\ndef get_fuse_location(username, file_name):\n  \"\"\"\n  This function creates filename localized with the username on the ML optimized FUSE mount point\n  \n  :param username: the databricks username \n  :param file_name: the name of the file  \n  :return: localized filename \n  \"\"\"\n  \n  path = \"/dbfs/ml/{}/ml_workshop\".format(username)\n  dbutils.fs.mkdirs(path)\n  \n  return \"{}/{}\".format(path, file_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"509f4a49-d5d8-41b3-84a3-cf54bb88aa48"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%fs ls dbfs:/ml-workshop-datasets/employee/delta/trainingData"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"85748369-4654-4275-8110-2dbb228aaf9f"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["%fs ls dbfs:/ml-workshop-datasets/employee/delta/testData"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f5125fa2-ef22-404b-abf9-c486c1a05d11"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##Data preparation logic"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6351b8aa-e6c0-415e-992b-480616693df3"}}},{"cell_type":"markdown","source":["### Data description recap\nBank client data:\n- age (numeric)\n- job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')\n- marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)\n- education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')\n- default: has credit in default? (categorical: 'no','yes','unknown')\n- housing: has housing loan? (categorical: 'no','yes','unknown')\n- loan: has personal loan? (categorical: 'no','yes','unknown')\n\nRelated with the last contact of the current campaign:\n- contact: contact communication type (categorical: 'cellular','telephone') \n- month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')\n- day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')\n- duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no'). Yet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.\n\nOther attributes:\n- campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)\n- pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)\n- previous: number of contacts performed before this campaign and for this client (numeric)\n- poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')\n\nOutput variable (desired target):\n21 - y - has the client subscribed a term deposit? (binary: 'yes','no')"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5f2a9daf-65ff-42a2-9528-941b3d7a8cb2"}}},{"cell_type":"markdown","source":["### Read data with Spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b8de2225-306b-448f-9b84-f8a652338aff"}}},{"cell_type":"code","source":["def read_data(path):\n  \"\"\"\n  This function read data data from s3 and drops columns that are not required for the modelling process\n  \n  :param path: the input path on dbfs \n  :return: a dataframe referencing the data on the input path\n  \"\"\"\n    \n  return spark.read.format(\"delta\")\\\n    .option(\"path\", path)\\\n    .load()\\\n    .drop(\"duration\")\\\n    .drop(\"features\")\\\n    .drop(\"label\")\n\n\ntrain_df = read_data(train_data_path)\ntest_df = read_data(test_data_path)  "],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"edb68931-a349-40a2-8cf2-2115892b9336"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["display(train_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9e796b91-6b42-4516-a6f3-8fd1981ce18d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Handle missing values, and generate features using Spark pipelines"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66477d8f-4463-4986-9699-c5df4c9f7b20"}}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import StandardScaler\nfrom pyspark.ml.linalg import Vectors\nfrom pyspark.ml.feature import VectorAssembler, OneHotEncoder, StringIndexer\nfrom pyspark.sql.functions import col, expr\n\ndef fixMissingValues(df):\n  \"\"\"\n  This function apply rules regarding missing values to the input dataframe\n  \n  :param df: the input dataframe \n  :return: a dataframe in which missing value rules have been applied\n  \"\"\"\n  \n  return (df\n  .withColumn(\"new_prospect\", expr(\"case when pdays = 999 then 1 else 0 end\"))\n  .withColumn(\"pdays_clean\", expr(\"case when pdays = 999 then 0 else pdays end\")))\n  \n  \ndef getNumericalFeaturesPipeline(features):\n  \"\"\"\n  This function creates a sub-pipeline that creates numerical features \n  \n  :param features: an array of column names containing numerical features \n  :return: a pipeline containing the logic for the numerical features \n  \"\"\"\n  \n  numerical_assembler = VectorAssembler(\n    inputCols=features,\n    outputCol=\"unscaledNumericalFeatures\")\n  \n  numericalScaler = StandardScaler(inputCol=\"unscaledNumericalFeatures\", outputCol=\"numericalFeatures\", withStd=True, withMean=False)\n  \n  return Pipeline(stages=[numerical_assembler, numericalScaler])\n  \n  \ndef getCategoricalFeaturesPipeline(features):\n  \"\"\"\n  This function creates a sub-pipeline that creates categorical features \n  \n  :param features: an array of column names containing categorical features\n  :return: a pipeline containing the logic for the categorical features \n  \"\"\"\n  \n  indexed_features = [feature + \"Indexed\" for feature in features]\n  onehot_features = [feature + \"OneHot\" for feature in features]\n  \n  indexers = [StringIndexer(inputCol=feature, outputCol=indexed_feature) for feature, indexed_feature in zip(features, indexed_features)]\n  onehot_encoders = OneHotEncoder(inputCols = indexed_features, outputCols = onehot_features)\n  categorical_assembler = VectorAssembler(inputCols=onehot_features, outputCol=\"categoricalFeatures\")\n  \n  return Pipeline(stages=indexers + [onehot_encoders] + [categorical_assembler])  \n\n\ndef getLabelPipeline(target_feature):\n  \"\"\"\n  This function creates a sub-pipeline that creates the label column \n  \n  :param target_feature: the name of the target feature to be predicted\n  :return: a pipeline containing the logic for creating a label column \n  \"\"\"\n  \n  indexer = StringIndexer(inputCol=target_feature, outputCol='label')\n  return Pipeline(stages=[indexer])\n  \n\ndef getDataPreparationPipeline(numerical_features, categorical_features, other_features, target_feature):\n  \"\"\"\n  This function creates the complete data preparation pipeline that creates both features and labels.\n  The pipeline is build using subpipelines for numerical and categorical features.\n  \n  :param numerical_features: an array of column names containing numerical features \n  :param categorical_features: an array of column names containing categorical features\n  :param other_features: an array of column names containing features that do not need processing\n  :param target_feature: the name of the target feature to be predicted\n  :return: a pipeline containing the logic for the complete data preparation \n  \"\"\"\n  \n  feature_assembler = VectorAssembler(\n    inputCols=[\"numericalFeatures\", \"categoricalFeatures\"] + other_features,\n    outputCol=\"features\")\n  \n  return Pipeline(stages=[getNumericalFeaturesPipeline(numerical_features),\n                          getCategoricalFeaturesPipeline(categorical_features),\n                          getLabelPipeline(target_feature),\n                          feature_assembler]) \n\n# Categories the input columns\nnumerical_features = [\"age\", \"balance\", \"pdays_clean\", \"previous\"]\ncategorical_features = [\"job\", \"education\", \"housing\", \"loan\", \"contact\", \"poutcome\"]\ntarget_feature = \"y\"\n\n# Apply missing value rules\ntrain_df, test_df = fixMissingValues(train_df), fixMissingValues(test_df)\n\n# Create and fit the data input pipeline\nfeaturesPipeline = getDataPreparationPipeline(\n  numerical_features, \n  categorical_features, \n  [\"new_prospect\"],\n  target_feature\n)\n\nfeaturesPipelineModel = featuresPipeline.fit(train_df)\n\n# Apply the input pipeline to both the train and test dataframes\ntrain_input_df = (featuresPipelineModel\n            .transform(train_df)\n            .select(\"features\", \"label\"))\n\ntest_input_df = (featuresPipelineModel\n            .transform(test_df)\n            .select(\"features\", \"label\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b2fe88d5-383e-4a34-982a-d8039990bc72"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Convert data to Pandas"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ca51be24-bb69-447c-a09e-40c94c2f20cd"}}},{"cell_type":"code","source":["import pandas as pd\nimport numpy as np\nimport os.path\n\n# Vectorized data is not supported by Arrow\nspark.conf.set(\"spark.sql.execution.arrow.enabled\", False)\n\ndef convert2numpy(df):\n  \"\"\"\n  This function converts the vectorized feature column and the label column of a Spark dataframe into a numpy matrix\n  \n  :param df: the input Spark dataframe \n  :return: a numpy matrix containing the features and label as columns \n  \"\"\"\n    \n  pandas_df = df.toPandas()\n  n = len(pandas_df)\n  series = pandas_df['features'].apply(lambda x : np.array(x.toArray())).values.reshape(-1,1)\n  X = np.apply_along_axis(lambda x : x[0], 1, series)\n  y = pandas_df['label'].values\n  data = np.concatenate((X, y.reshape(-1,1)), axis=1)\n  return data"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"07bb2f3b-9822-4f2e-b9d1-7a26dc15f8fc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Write data to DBFS"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1bc9be4c-4fab-4494-8ba4-f15d30d1bc95"}}},{"cell_type":"code","source":["# Convert the data to numpy\ntrain_data = convert2numpy(train_input_df)\nvalidation_data = convert2numpy(test_input_df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6d081638-283f-4c41-b056-53fca4d9cc9e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Save to dbfs\npath = \"/dbfs/ml-workshop-datasets/employee/numpy\"\nif not os.path.exists(path):\n  os.makedirs(path)\nnp.save(\"{}/{}\".format(path, \"train.npy\"), train_data)\nnp.save(\"{}/{}\".format(path, \"test.npy\"), validation_data)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7153aa96-6c59-4852-ab28-5915e993c417"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["n = train_data.shape[0]\nm = train_data.shape[1]\n\nassert m == validation_data.shape[1], \"The train and test data must have the same number of features\""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"832bdd29-8a03-4ccc-a105-9cd5aa7364f1"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e174577e-1ac2-43ce-95c6-cdef7d9297b3"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"02 Feature Engineering for Distributed HyperOpt","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3256439173735693}},"nbformat":4,"nbformat_minor":0}
