{"cells":[{"cell_type":"markdown","source":["The dataset used for this example is Bank marketing. Given a set of features about a customer can we predict whether the person will open a term deposit account.\n\nOriginal Source: [UCI Machine Learning Repository \nBank Marketing Data Set](https://archive.ics.uci.edu/ml/datasets/bank+marketing)\n[Moro et al., 2014] S. Moro, P. Cortez and P. Rita. A Data-Driven Approach to Predict the Success of Bank Telemarketing. Decision Support Systems, Elsevier, 62:22-31, June 2014"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9c3c3a2-b185-41bc-be05-d3052eabf2ca"}}},{"cell_type":"markdown","source":["### Deploying our model to AWS Sagemaker\nIn this notebook we will deploy our model from \"production\" stage in model registry to AWS Sagemaker. In order for this notebook to work, a few prerequisites must be met:\n* First we need to build a docker image that will work with our MLFlow model. To do this:\n  * Install docker and MLFlow on your local machine\n  * use `mlflow sagemaker build-and-push --build` to create an MLFlow image that will work with Sagemaker\n* Secondly, the docker image must be uploaded to AWS Elastic Container Registry (ECR)\n  * In your AWS account, create a new ECR repository\n  * [Install the AWS CLI](https://docs.aws.amazon.com/cli/latest/userguide/install-cliv2.html), and subsequently [configure credentials](https://docs.aws.amazon.com/cli/latest/reference/configure/)\n  * Subsequently, [you must authenticate docker with your ECR repository](https://docs.aws.amazon.com/AmazonECR/latest/userguide/Registries.html#registry_auth)\n  * You must also make sure the AWS [user used for AWS CLI has access to ECR](https://docs.aws.amazon.com/AmazonECR/latest/userguide/security_iam_id-based-policy-examples.html).\n  * Then, use `docker tag` to rename your image to the ECR repository link.\n  * Finally, use `docker push` to push the image to your ECR repository\n* Third, we need to make sure our Databricks cluster is authenticated with AWS Sagemaker:\n  * To do this, get or create an IAM role that has sagemaker permissions. AmazonSageMakerFullAccess role in AWS IAM is readily available\n  * Secondly, add this role to the cross-account role that has been defined when setting up Databricks.\n    * [this link](https://docs.databricks.com/administration-guide/cloud-configurations/aws/instance-profiles.html) roughly shows the steps. Instead of adding an S3 IAM role, you instead add the AmazonSageMakerFullAccess role.\n  * Lastly, we need to add the role ARN to \"instance profiles\" in the databricks admin console.\n  * Now we can launch a databricks cluster with our AWS Sagemaker, which means that our cluster has access to Sagemaker resources.\n  \nWhile this is a lot to set up, remember that all the above actions have to be executed only once. After all, we only really need one MLFlow image in ECR, and authentication also only has to be set-up once"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ebb9cbf4-17a0-4230-9e56-84bb8a1f9ee2"}}},{"cell_type":"markdown","source":["### Step 1: From Model Registry, get the URI of our production model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"29fece90-1523-4d7c-8736-9110030643d7"}}},{"cell_type":"code","source":["import mlflow\nfrom mlflow.tracking import MlflowClient\nclient = MlflowClient()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8e392e76-bc06-4c12-9710-7bc6e99b7603"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["#dbutils.widgets.text(\"modelRegistryName\",\"bankXGBoost\")\nmodelRegistryName = dbutils.widgets.get(\"modelRegistryName\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3e5845a4-68fc-429e-88ad-195746a6635e"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["def getProdModelURI(modelRegistryName):\n  models = client.search_model_versions(\"name='%s'\" % modelRegistryName)\n  source = [model for model in models if model.current_stage == \"Production\"][0].source\n  return source\n\nmodelURI = getProdModelURI(modelRegistryName)\n# latest_prod_model_detail = client.get_latest_versions(model_name, stages=['Production'])[0]"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21433174-391c-439f-b10e-4c66aa158dca"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 2: Deploy our model to Sagemaker"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"78911f59-7759-4407-8282-703ee9f6a8e3"}}},{"cell_type":"markdown","source":["In the cell below we define what we want to call our Sagemaker app, and we get the mlflow image that has been registered to AWS ECR. We use mode \"replace\" so that we can overwrite our model in subsequent iterations."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0d6e336b-9f3b-408e-b428-af2e03179319"}}},{"cell_type":"code","source":["app_name = \"xgboostBank\"\nimage_url = \"*******.dkr.ecr.eu-west-1.amazonaws.com/mthone:latest\"\n\nimport mlflow.sagemaker as mfs\nmfs.deploy(app_name=app_name, model_uri=modelURI, image_url = image_url, region_name=\"eu-west-1\", mode=\"replace\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ca5fe2a-99b0-491f-b00c-dbf4a7f9ffe6"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 3: Querying our deployed model"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a3fc6ce6-f661-4901-9599-dc5d57e9a15d"}}},{"cell_type":"code","source":["df = spark.sql(\"select * from max_db.bank_marketing_train_set\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"865417ab-f465-4398-9b47-5b81bdb650c7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["train = df.toPandas()\ntrain_x = train.drop([\"label\"], axis=1)\nsample = train_x.iloc[[0]]\n#sample = train_x.iloc[:, :]\nsample_json = sample.to_json(orient=\"split\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b063a483-29eb-49ec-bafb-a04d7db69d46"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["sample_json"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"aac1c07b-570d-4a09-93d7-5e22328c1265"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["import json\nimport boto3\ndef query_endpoint_example(app_name, input_json):\n  print(\"Sending batch prediction request with inputs: {}\".format(input_json))\n  client = boto3.session.Session().client(\"sagemaker-runtime\", \"eu-west-1\")\n  \n  response = client.invoke_endpoint(\n      EndpointName=app_name,\n      Body=input_json,\n      ContentType='application/json; format=pandas-split',\n  )\n  preds = response['Body'].read().decode(\"ascii\")\n  preds = json.loads(preds)\n  print(\"Received response: {}\".format(preds))\n  return preds\n\nimport pandas as pd\n#input_df = pd.DataFrame([query_input])\n#input_json = input_df.to_json(orient='split')\n\nprediction1 = query_endpoint_example(app_name=app_name, input_json=sample_json)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e936931d-365f-46be-8ed2-fef3cafa9fef"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Step 4: Clean up our Sagemaker deployment\nIt is important to delete your Sagemaker deployment when you no longer use it, as it makes use of permanently running EC2 instances which will incur costs. For more information see [this link](https://aws.amazon.com/sagemaker/pricing/)."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6e1f3151-3bfd-4661-8d10-214d6fa0ac49"}}},{"cell_type":"code","source":["mfs.delete(app_name=app_name, region_name=\"eu-west-1\", archive=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52898a46-4542-4d2c-b95c-89eb53cf2501"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Extra: A note on scheduling with a sagemaker deployment\n* Scheduling can be done similar to notebook 3.1, using the Databricks scheduler. Once a deployment has been done, all you need to run is the Step 3 section."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c06c1dcd-3ed7-40cd-923e-24e36ad98cc9"}}},{"cell_type":"code","source":[""],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"41886c0e-0c2a-4fa8-a9ce-94fdcdd0b740"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"02 DeployToSageMaker","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3256439173735550}},"nbformat":4,"nbformat_minor":0}
